{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Learnings\\College_project\\chat\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import AgentExecutor, Tool, create_tool_calling_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Environment Variables from .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the GROQ_API_KEY environment variable to the value retrieved from .env file\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Set the COHERE_API_KEY environment variable to the value retrieved from .env file\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# Set the PINECONE_API_KEY environment variable to the value retrieved from .env file\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Enable Langsmith tracking by setting the LANGCHAIN_TRACING_V2 environment variable to \"true\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Set the LANGCHAIN_API_KEY environment variable to the value retrieved from .env file\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a Wikipedia API wrapper with customized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Wikipedia tool\n",
    "wiki_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=1,\n",
    "        load_all_available_meta=False,\n",
    "        doc_content_chars_max=500\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a tool object for accessing Wikipedia information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tool= Tool(\n",
    "        name='Wikipedia',\n",
    "        description='look up things in wikipedia for knowing about food recipes, cooking instructions and their history',\n",
    "        func=wiki_tool.invoke\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Path to 'data' Directory Located in Parent Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_path = Path().resolve()\n",
    "\n",
    "# Get the parent directory of the current working directory\n",
    "parent_path = current_path.parent\n",
    "\n",
    "# Construct the path to the 'data' directory located in the parent directory\n",
    "data_path = parent_path / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through files in the specified data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = []  # Initialize an empty list to store PDF files.\n",
    "for file in data_path.iterdir():  # Iterate through each file in the data path.\n",
    "    if file.is_file() and file.name.endswith('.pdf'):  # Check if the file is a regular file and has a .pdf extension.\n",
    "        pdf_files.append(file)  # If the file meets the criteria, append it to the list of PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('E:/Learnings/College_project/data/BHM-401T.pdf'),\n",
       " WindowsPath('E:/Learnings/College_project/data/Book1.pdf'),\n",
       " WindowsPath('E:/Learnings/College_project/data/Professional_Cooking.pdf'),\n",
       " WindowsPath('E:/Learnings/College_project/data/USU-Student-Cookbook-FINAL-1.pdf')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading PDF Documents into a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []  # Initialize an empty list to store loaded documents.\n",
    "\n",
    "for filepath in pdf_files:  # Iterate through each filepath in the list of PDF files.\n",
    "    loader = PyPDFLoader(filepath)  # Create a PyPDFLoader object for the current filepath.\n",
    "    docs.append(loader.load())  # Load the document using the PyPDFLoader and append it to the list of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Loaded PDF Documents into Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []  # Initialize an empty list to store the chunks of documents.\n",
    "\n",
    "# Initialize a text splitter with specified chunk size and overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "\n",
    "for doc in docs:  # Iterate through each loaded document in the docs list.\n",
    "    splitted_docs = text_splitter.split_documents(doc)  # Split the document into chunks using the text splitter.\n",
    "    documents.append(splitted_docs)  # Append the list of chunks to the documents list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Embedding Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model= CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Page Contents from Document Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []  # Initialize an empty list to store page contents of documents.\n",
    "\n",
    "for document in documents:  # Iterate through each document chunk in the documents list.\n",
    "    page_content = []  # Initialize an empty list to store page contents of the current document.\n",
    "    \n",
    "    # Iterate through each page in the current document chunk.\n",
    "    for page in range(0, len(document)):\n",
    "        page_content.append(document[page].page_content)  # Append the page content to the page_content list.\n",
    "    \n",
    "    contents.append(page_content)  # Append the page_content list to the contents list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Pinecone \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Pinecone Index Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the Pinecone index.\n",
    "index_name = \"rasoiguru\"  \n",
    "# Retrieve the cloud provider from environment variables or default to 'aws'.\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws' \n",
    "# Retrieve the region from environment variables or default to 'us-east-1'.\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'  \n",
    "# Create a serverless specification object with the specified cloud and region.\n",
    "spec = ServerlessSpec(cloud=cloud, region=region) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking and Creating Pinecone Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pc.list_indexes().names():  # Check if the specified index name does not exist.\n",
    "    # Create a new index with specified parameters.\n",
    "    pc.create_index(\n",
    "        index_name,  # Specify the name of the index to be created.\n",
    "        dimension=4096,  # Specify the dimension of the vector embeddings.\n",
    "        metric='cosine',  # Specify the distance metric for similarity search.\n",
    "        spec=spec  # Specify the serverless specification for deployment.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing and Describing Pinecone Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 4096,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)  # Access the Pinecone index with the specified index name.\n",
    "\n",
    "# Wait briefly for connection before describing index stats.\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()  # Describe the statistics of the Pinecone index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Namespace Names for PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate namespace names for each PDF file.\n",
    "ns = [\"ns\" + path.stem for path in pdf_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating or Retrieving Vector Stores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n",
      "Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
     ]
    },
    {
     "ename": "TooManyRequestsError",
     "evalue": "status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mdescribe_index_stats()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_vector_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Check if no vectors are present in the index.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Create vector stores from document contents.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m namespace, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ns, contents):  \u001b[38;5;66;03m# Iterate through namespaces and corresponding document contents.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mPineconeVectorStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Provide the document contents as texts.\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the name of the Pinecone index.\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the embedding model for generating vectors.\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the namespace for the vector store.\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m         vectorstores\u001b[38;5;241m.\u001b[39mappend(vectorstore)  \u001b[38;5;66;03m# Append the created vector store to the list.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Retrieve vector stores from an existing index.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\langchain_pinecone\\vectorstores.py:441\u001b[0m, in \u001b[0;36mPineconeVectorStore.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m pinecone_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_pinecone_index(index_name, pool_threads)\n\u001b[0;32m    439\u001b[0m pinecone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(pinecone_index, embedding, text_key, namespace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 441\u001b[0m pinecone\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    442\u001b[0m     texts,\n\u001b[0;32m    443\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    444\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    445\u001b[0m     namespace\u001b[38;5;241m=\u001b[39mnamespace,\n\u001b[0;32m    446\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    447\u001b[0m     embedding_chunk_size\u001b[38;5;241m=\u001b[39membeddings_chunk_size,\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(upsert_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    449\u001b[0m )\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pinecone\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\langchain_pinecone\\vectorstores.py:157\u001b[0m, in \u001b[0;36mPineconeVectorStore.add_texts\u001b[1;34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m chunk_ids \u001b[38;5;241m=\u001b[39m ids[i : i \u001b[38;5;241m+\u001b[39m embedding_chunk_size]\n\u001b[0;32m    156\u001b[0m chunk_metadatas \u001b[38;5;241m=\u001b[39m metadatas[i : i \u001b[38;5;241m+\u001b[39m embedding_chunk_size]\n\u001b[1;32m--> 157\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m async_res \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    160\u001b[0m         vectors\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m ]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\langchain_cohere\\embeddings.py:151\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of document texts.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m        List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\langchain_cohere\\embeddings.py:118\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed\u001b[1;34m(self, texts, input_type)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    114\u001b[0m     texts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    116\u001b[0m     input_type: typing\u001b[38;5;241m.\u001b[39mOptional[cohere\u001b[38;5;241m.\u001b[39mEmbedInputType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m--> 118\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, e)) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m embeddings]\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\langchain_cohere\\embeddings.py:100\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39membed(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _embed_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\tenacity\\__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    416\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\tenacity\\__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\langchain_cohere\\embeddings.py:98\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed_with_retry.<locals>._embed_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39membed(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\cohere\\client.py:206\u001b[0m, in \u001b[0;36mClient.embed\u001b[1;34m(self, texts, images, model, input_type, embedding_types, truncate, request_options, batching)\u001b[0m\n\u001b[0;32m    203\u001b[0m textsarr: typing\u001b[38;5;241m.\u001b[39mSequence[\u001b[38;5;28mstr\u001b[39m]  \u001b[38;5;241m=\u001b[39m texts \u001b[38;5;28;01mif\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OMIT \u001b[38;5;129;01mand\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    204\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [textsarr[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(textsarr), embed_batch_size)]\n\u001b[1;32m--> 206\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    207\u001b[0m     response\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[0;32m    210\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m             texts\u001b[38;5;241m=\u001b[39mtext_batch,\n\u001b[0;32m    212\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    213\u001b[0m             input_type\u001b[38;5;241m=\u001b[39minput_type,\n\u001b[0;32m    214\u001b[0m             embedding_types\u001b[38;5;241m=\u001b[39membedding_types,\n\u001b[0;32m    215\u001b[0m             truncate\u001b[38;5;241m=\u001b[39mtruncate,\n\u001b[0;32m    216\u001b[0m             request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[0;32m    217\u001b[0m         ),\n\u001b[0;32m    218\u001b[0m         texts_batches,\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    220\u001b[0m ]\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\cohere\\client.py:206\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    203\u001b[0m textsarr: typing\u001b[38;5;241m.\u001b[39mSequence[\u001b[38;5;28mstr\u001b[39m]  \u001b[38;5;241m=\u001b[39m texts \u001b[38;5;28;01mif\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OMIT \u001b[38;5;129;01mand\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    204\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [textsarr[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(textsarr), embed_batch_size)]\n\u001b[1;32m--> 206\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    207\u001b[0m     response\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[0;32m    210\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m             texts\u001b[38;5;241m=\u001b[39mtext_batch,\n\u001b[0;32m    212\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    213\u001b[0m             input_type\u001b[38;5;241m=\u001b[39minput_type,\n\u001b[0;32m    214\u001b[0m             embedding_types\u001b[38;5;241m=\u001b[39membedding_types,\n\u001b[0;32m    215\u001b[0m             truncate\u001b[38;5;241m=\u001b[39mtruncate,\n\u001b[0;32m    216\u001b[0m             request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[0;32m    217\u001b[0m         ),\n\u001b[0;32m    218\u001b[0m         texts_batches,\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    220\u001b[0m ]\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\concurrent\\futures\\_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\concurrent\\futures\\_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\cohere\\client.py:209\u001b[0m, in \u001b[0;36mClient.embed.<locals>.<lambda>\u001b[1;34m(text_batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m textsarr: typing\u001b[38;5;241m.\u001b[39mSequence[\u001b[38;5;28mstr\u001b[39m]  \u001b[38;5;241m=\u001b[39m texts \u001b[38;5;28;01mif\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OMIT \u001b[38;5;129;01mand\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    204\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [textsarr[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(textsarr), embed_batch_size)]\n\u001b[0;32m    206\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    207\u001b[0m     response\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m--> 209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: \u001b[43mBaseCohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43membedding_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    218\u001b[0m         texts_batches,\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    220\u001b[0m ]\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[1;32me:\\Learnings\\College_project\\chat\\lib\\site-packages\\cohere\\base_client.py:1887\u001b[0m, in \u001b[0;36mBaseCohere.embed\u001b[1;34m(self, texts, images, model, input_type, embedding_types, truncate, request_options)\u001b[0m\n\u001b[0;32m   1877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnprocessableEntityError(\n\u001b[0;32m   1878\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(\n\u001b[0;32m   1879\u001b[0m             typing\u001b[38;5;241m.\u001b[39mOptional[typing\u001b[38;5;241m.\u001b[39mAny],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1884\u001b[0m         )\n\u001b[0;32m   1885\u001b[0m     )\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m-> 1887\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequestsError(\n\u001b[0;32m   1888\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(\n\u001b[0;32m   1889\u001b[0m             typing\u001b[38;5;241m.\u001b[39mOptional[typing\u001b[38;5;241m.\u001b[39mAny],\n\u001b[0;32m   1890\u001b[0m             construct_type(\n\u001b[0;32m   1891\u001b[0m                 type_\u001b[38;5;241m=\u001b[39mtyping\u001b[38;5;241m.\u001b[39mOptional[typing\u001b[38;5;241m.\u001b[39mAny],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m                 object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[0;32m   1893\u001b[0m             ),\n\u001b[0;32m   1894\u001b[0m         )\n\u001b[0;32m   1895\u001b[0m     )\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m498\u001b[39m:\n\u001b[0;32m   1897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidTokenError(\n\u001b[0;32m   1898\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(\n\u001b[0;32m   1899\u001b[0m             typing\u001b[38;5;241m.\u001b[39mOptional[typing\u001b[38;5;241m.\u001b[39mAny],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1904\u001b[0m         )\n\u001b[0;32m   1905\u001b[0m     )\n",
      "\u001b[1;31mTooManyRequestsError\u001b[0m: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}"
     ]
    }
   ],
   "source": [
    "vectorstores = []  # Initialize an empty list to store vector stores.\n",
    "\n",
    "if index.describe_index_stats()['total_vector_count'] == 0:  # Check if no vectors are present in the index.\n",
    "    # Create vector stores from document contents.\n",
    "    for namespace, content in zip(ns, contents):  # Iterate through namespaces and corresponding document contents.\n",
    "        vectorstore = PineconeVectorStore.from_texts(\n",
    "            texts=content,  # Provide the document contents as texts.\n",
    "            index_name=index_name,  # Specify the name of the Pinecone index.\n",
    "            embedding=embedding_model,  # Specify the embedding model for generating vectors.\n",
    "            namespace=namespace  # Specify the namespace for the vector store.\n",
    "        )\n",
    "        vectorstores.append(vectorstore)  # Append the created vector store to the list.\n",
    "\n",
    "else:\n",
    "    # Retrieve vector stores from an existing index.\n",
    "    for namespace in ns:  # Iterate through namespaces.\n",
    "        vectorstore = PineconeVectorStore.from_existing_index(\n",
    "            index_name,  # Specify the name of the Pinecone index.\n",
    "            embedding_model,  # Specify the embedding model used in the index.\n",
    "            namespace=namespace  # Specify the namespace for the vector store.\n",
    "        )\n",
    "        vectorstores.append(vectorstore)  # Append the retrieved vector store to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<langchain_pinecone.vectorstores.PineconeVectorStore at 0x299cb75b130>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Vector Stores to Retrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers = []  # Initialize an empty list to store retrievers.\n",
    "\n",
    "for vectorstore in vectorstores:  # Iterate through each vector store.\n",
    "    retriever = vectorstore.as_retriever()  # Convert the vector store to a retriever.\n",
    "    retrievers.append(retriever)  # Append the retriever to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorStoreRetriever(tags=['PineconeVectorStore', 'CohereEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x00000299CB75B130>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Search Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []  # Initialize an empty list to store search tools.\n",
    "\n",
    "# Lists containing tool names and descriptions\n",
    "tools_name = [\n",
    "    'BHM-401T_pdf_search',\n",
    "    'Book1_pdf_search',\n",
    "    'Professional_Cooking_pdf_search',\n",
    "    'USU-Student-Cookbook-FINAL-1_pdf_search'\n",
    "]\n",
    "\n",
    "tools_desc = [\n",
    "    \"Indian food cooking and heritage related information use this tool\",\n",
    "    \"For information related to any ingredient use this tool\",\n",
    "    \"For professional cooking techniques, sanitization and safety in kitchen and food presentation tips use this tool\",\n",
    "    \"For specific information about quick recipes for students and seasonal grocery shopping use this tool\"\n",
    "]\n",
    "\n",
    "# Create search tools for each retriever and append them to the tools list\n",
    "for name, desc, retv in zip(tools_name, tools_desc, retrievers):\n",
    "    pdf_tool = create_retriever_tool(retv, name, desc, document_prompt=\"Search the query\")  # Create a search tool for each retriever\n",
    "    tools.append(pdf_tool)  # Append the search tool to the tools list\n",
    "\n",
    "# Append the Wikipedia tool to the tools list\n",
    "tools.append(wiki_tool)  # Append the Wikipedia tool to the tools list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Large Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting System Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction= \"\"\"\n",
    "You are a helpful cooking assistant named Rasoiguru.\\\n",
    "Greet the user\\\n",
    "Answer the following questions as best you can in terms of a passionate and helpful  professional cooking assistant\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Format Definition for Agent Responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "format= \"\"\"\n",
    "Use the following format:\n",
    "\n",
    "Use the chat history which will be provided to you for understanding the context of the most recent conversation incase user query is not clearly defined\\\n",
    "Question: the input question you must answer\\\n",
    "Thought: you should always think about what to do\\\n",
    "Action: the action to take, should be one of the provided tools\\\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "## You need to always give an final answer\n",
    "\n",
    "Remember to answer as a compansionate professional cooking assistant when giving your final answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"\"\" You have access to the following tools:\n",
    "Tools:\n",
    "{tools}\n",
    "Instruction:\n",
    "{system_instruction}.\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"Begin!Now answer the question\n",
    "{intermediate_steps}\n",
    "Chat history:\n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "## In case the user query is not about food cooking, grocery shopping and history of food the  reply\\\n",
    "then reply I do not know the answer to your question.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"chat_history\", \"intermediate_steps\", \"agent_scratchpad\"],\n",
    "    template= prefix + format + suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input', 'intermediate_steps'], template=\" You have access to the following tools:\\nTools:\\n[Tool(name='BHM-401T_pdf_search', description='Indian food cooking and heritage related information use this tool', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000299B52681F0>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'CohereEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x00000299CB75B130>), document_prompt='Search the query', document_separator='\\\\n\\\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000299B52683A0>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'CohereEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x00000299CB75B130>), document_prompt='Search the query', document_separator='\\\\n\\\\n')), Tool(name='Wikipedia', description='look up things in wikipedia for knowing about food recipes, cooking instructions and their history', func=<bound method BaseTool.invoke of WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'e:\\\\\\\\Learnings\\\\\\\\College_project\\\\\\\\chat\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\wikipedia\\\\\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=500))>)]\\nInstruction:\\n\\nYou are a helpful cooking assistant named Rasoiguru.Greet the userAnswer the following questions as best you can in terms of a passionate and helpful  professional cooking assistant.\\n\\nUse the following format:\\n\\nUse the chat history which will be provided to you for understanding the context of the most recent conversation incase user query is not clearly definedQuestion: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of the provided toolsAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n## You need to always give an final answer\\n\\nRemember to answer as a compansionate professional cooking assistant when giving your final answer.\\nBegin!Now answer the question\\n{intermediate_steps}\\nChat history:\\n{chat_history}\\nQuestion: {input}\\n{agent_scratchpad}\\n## In case the user query is not about food cooking, grocery shopping and history of food the  replythen reply I do not know the answer to your question.\\n\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Creation with Tool Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= create_tool_calling_agent(llm,tools= tools,  prompt= prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Integration for Agent Interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory= ConversationBufferWindowMemory(k=3,return_messages=True, memory_key= \"chat_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Executor Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Executor: Task Invocation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Wikipedia` with `sandwich`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Sandwich\n",
      "Summary: A sandwich is a dish typically consisting of meat, cheese or vegetables used as a filling between slices of bread, or placed atop a slice of bread; or, more generally, any dish in which bread serves as a container or wrapper for another food type, and allows it to be a finger food. The sandwich began as a portable, convenient food in the Western world, though over time it has become prevalent worldwide.\n",
      "There has been social media debate over the precise definition of san\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Wikipedia` with `sandwich preparation`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Peanut butter and jelly sandwich\n",
      "Summary: A peanut butter and jelly sandwich (PB&J) consists of peanut butter and fruit preserves spread on bread. The sandwich is popular in the United States, especially among children; a 2002 survey showed the average American will eat 1,500 peanut butter and jelly sandwiches before graduating from high school. There are many variations of the PB&J, which itself is a hybrid between a peanut butter sandwich and a jam sandwich.\n",
      "In American terminology, jell\u001b[0m\u001b[32;1m\u001b[1;3mThought: The user is asking how to make a sandwich. Based on the chat history, I see that we have already gathered information about sandwiches in general and the preparation of sandwiches from the Wikipedia tool. Now, I can use this information to provide a helpful and detailed response about making a sandwich.\n",
      "\n",
      "Final Answer: To make a sandwich, you'll need two slices of bread as a base. You can then choose your desired fillings - this could be meat, cheese, or vegetables. Spread your chosen fillings between the slices of bread, or place them on top of one slice. Bread serves as a container or wrapper for the fillings, making the sandwich a convenient and portable food. You can customize your sandwich to your liking by experimenting with various ingredients and flavor combinations. Enjoy your homemade sandwich!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\": \"How to make sandwich\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To make a sandwich, you'll need two slices of bread as a base. You can then choose your desired fillings - this could be meat, cheese, or vegetables. Spread your chosen fillings between the slices of bread, or place them on top of one slice. Bread serves as a container or wrapper for the fillings, making the sandwich a convenient and portable food. You can customize your sandwich to your liking by experimenting with various ingredients and flavor combinations. Enjoy your homemade sandwich!\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the Final Answer from a Result String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'output' field from the 'result' dictionary to a string\n",
    "result = str(result)\n",
    "\n",
    "# Define the marker indicating the start of the final answer\n",
    "start_marker = \"Final Answer:\"\n",
    "\n",
    "# Find the index where the start_marker appears in the result string\n",
    "start_index = result.find(start_marker)\n",
    "print(start_index)  # Print the index of the start_marker (for debugging purposes)\n",
    "\n",
    "# Check if the start_marker is found in the result string\n",
    "if start_index != -1:\n",
    "    # Extract the substring that comes after the start_marker and strip any leading/trailing whitespace\n",
    "    result = result[start_index + len(start_marker):].strip()\n",
    "else:\n",
    "    # If the start_marker is not found, set result to an empty string\n",
    "    result = \"\"\n",
    "\n",
    "# Print the final extracted result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
